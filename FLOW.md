# Code Flow Documentation

This document details the architecture, data flow, and component interactions in the LLM-Powered Resume Screening System.

## ðŸ“ Architecture Overview

The system follows a multi-agent architecture with two main evaluation paths:

1. **Orchestrator Flow**: Quick rule-based matching for initial screening
2. **Structured Scoring Flow**: Comprehensive LLM-based evaluation with weighted criteria

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Entry Point: pipeline.py                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚
        â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestrator  â”‚          â”‚ Structured Scoring    â”‚
â”‚   Flow        â”‚          â”‚   Agent (Batch)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                              â”‚
        â”‚                              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Gap Analysis    â”‚
              â”‚  & Ranking      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”„ Detailed Flow

### Phase 1: Orchestrator Flow (`main_orchestrator.py`)

**Purpose**: Quick initial matching using rule-based logic

```
pipeline.py::run_pipeline()
    â”‚
    â”œâ”€â”€> main_orchestrator.run(job_pdf, resume_pdfs)
    â”‚       â”‚
    â”‚       â”œâ”€â”€> JobDescriptionParserAgent.parse(job_pdf)
    â”‚       â”‚       â”‚
    â”‚       â”‚       â”œâ”€â”€> services/pdf_utils.extract_text_from_pdf()
    â”‚       â”‚       â”‚       â””â”€â”€> PyPDF2.PdfReader() extracts text
    â”‚       â”‚       â”‚
    â”‚       â”‚       â”œâ”€â”€> services/llm.LLMService.generate()
    â”‚       â”‚       â”‚       â””â”€â”€> LangChain + Google Gemini API
    â”‚       â”‚       â”‚
    â”‚       â”‚       â””â”€â”€> Returns: JobCriteria (Pydantic model)
    â”‚       â”‚               - position
    â”‚       â”‚               - required_skills[]
    â”‚       â”‚               - preferred_skills[]
    â”‚       â”‚               - min_experience_years
    â”‚       â”‚               - education_level
    â”‚       â”‚               - industry, company_size, remote_work
    â”‚       â”‚
    â”‚       â”œâ”€â”€> ResumeScreenerAgent.parse(resume_pdf) [for each resume]
    â”‚       â”‚       â”‚
    â”‚       â”‚       â”œâ”€â”€> services/pdf_utils.extract_text_from_pdf()
    â”‚       â”‚       â”‚
    â”‚       â”‚       â”œâ”€â”€> services/llm.LLMService.generate()
    â”‚       â”‚       â”‚       â””â”€â”€> LLM extracts:
    â”‚       â”‚       â”‚           - extracted_skills[]
    â”‚       â”‚       â”‚           - education_level
    â”‚       â”‚       â”‚           - total_experience_years (parsed from dates)
    â”‚       â”‚       â”‚
    â”‚       â”‚       â””â”€â”€> Returns: ResumeData (Pydantic model)
    â”‚       â”‚               - file_path
    â”‚       â”‚               - raw_text
    â”‚       â”‚               - extracted_skills[]
    â”‚       â”‚               - education_level
    â”‚       â”‚               - total_experience_years
    â”‚       â”‚
    â”‚       â””â”€â”€> MatchmakerAgent.score(resume, job) [for each resume]
    â”‚               â”‚
    â”‚               â”œâ”€â”€> Skills Matching
    â”‚               â”‚       â””â”€â”€> Set intersection: (job_skills âˆ© resume_skills) / job_skills
    â”‚               â”‚
    â”‚               â”œâ”€â”€> Experience Matching
    â”‚               â”‚       â””â”€â”€> Compares resume.total_experience_years vs job.min_experience_years
    â”‚               â”‚           - If resume_exp >= job_required: 100 points
    â”‚               â”‚           - If resume_exp > 0 but < required: 40-90 points (proportional)
    â”‚               â”‚           - If resume_exp = 0 but job requires: 30 points
    â”‚               â”‚
    â”‚               â”œâ”€â”€> Education Matching
    â”‚               â”‚       â””â”€â”€> Compares education levels (high_school < bachelors < masters < doctorate)
    â”‚               â”‚
    â”‚               â””â”€â”€> Returns: MatchResult
    â”‚                       - job: JobCriteria
    â”‚                       - resume: ResumeData
    â”‚                       - scores: MatchScores (skills_match, experience_match, education_match, overall_score)
    â”‚
    â””â”€â”€> Returns: List[MatchResult] (one per resume)
```

### Phase 2: Structured Scoring Flow (`structured_scoring_agent.py`)

**Purpose**: Comprehensive LLM-based evaluation with weighted criteria

```
pipeline.py::run_pipeline()
    â”‚
    â”œâ”€â”€> StructuredScoringAgent.score_resumes_batch(resume_paths, job_description_path, criteria_requirements)
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Extract text from all PDFs (parallel)
    â”‚       â”‚       â”œâ”€â”€> Extract job description text
    â”‚       â”‚       â””â”€â”€> Extract all resume texts
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Create batch scoring prompt
    â”‚       â”‚       â””â”€â”€> structured_scoring_agent.create_batch_scoring_prompt()
    â”‚       â”‚               - Includes all resumes for comparative evaluation
    â”‚       â”‚               - Includes job description
    â”‚       â”‚               - Includes criteria weights
    â”‚       â”‚               - Instructs LLM to score relatively across candidates
    â”‚       â”‚
    â”‚       â”œâ”€â”€> LLM Batch Evaluation
    â”‚       â”‚       â””â”€â”€> Single LLM call evaluates all resumes together
    â”‚       â”‚               - Returns JSON with scores for each resume
    â”‚       â”‚               - Each resume scored on all criteria:
    â”‚       â”‚                 * raw_score (0-100)
    â”‚       â”‚                 * weight_given
    â”‚       â”‚                 * normalized_percentage
    â”‚       â”‚                 * weighted_contribution
    â”‚       â”‚               - total_score (sum of weighted contributions)
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Parse LLM Response
    â”‚       â”‚       â””â”€â”€> structured_scoring_agent.parse_llm_response()
    â”‚       â”‚               - Extracts JSON from LLM response
    â”‚       â”‚               - Handles markdown code blocks
    â”‚       â”‚               - Error handling with fallback
    â”‚       â”‚
    â”‚       â””â”€â”€> Returns: List[Dict[str, Any]]
    â”‚               Each dict contains:
    â”‚               - success: bool
    â”‚               - scoring_result: {
    â”‚                   "technical_skills": {...},
    â”‚                   "experience": {...},
    â”‚                   "education": {...},
    â”‚                   ... (other criteria)
    â”‚                   "total_score": float,
    â”‚                   "metadata": {...}
    â”‚                 }
    â”‚
    â””â”€â”€> Fallback (if batch fails)
            â””â”€â”€> _fallback_individual_scoring()
                    â””â”€â”€> Scores each resume individually (sequential)
```

### Phase 3: Gap Analysis & Enrichment (`pipeline.py`)

**Purpose**: Combine orchestrator and structured scoring results for gap analysis

```
pipeline.py::run_pipeline()
    â”‚
    â”œâ”€â”€> For each resume result:
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Extract MatchResult from orchestrator results
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Calculate Missing Skills
    â”‚       â”‚       â””â”€â”€> Set difference: job.required_skills - resume.extracted_skills
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Calculate Experience Gap
    â”‚       â”‚       â””â”€â”€> max(0, job.min_experience_years - resume.total_experience_years)
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Calculate Education Gap
    â”‚       â”‚       â””â”€â”€> Check if resume.education_level >= job.education_level
    â”‚       â”‚
    â”‚       â””â”€â”€> Add gap_analysis to scoring_result
    â”‚               {
    â”‚                 "missing_skills": [...],
    â”‚                 "experience_gap_years": float | None,
    â”‚                 "education_gap": str | None
    â”‚               }
    â”‚
    â””â”€â”€> Display results and save to outputs/
```

### Phase 4: Output Generation (`pipeline.py`)

```
pipeline.py::run_pipeline()
    â”‚
    â”œâ”€â”€> Rank candidates by total_score (descending)
    â”‚
    â”œâ”€â”€> Save to outputs/scoring_results.json
    â”‚       â””â”€â”€> Full structured results
    â”‚
    â”œâ”€â”€> Save to outputs/scoring_results_YYYYMMDD_HHMMSS.json
    â”‚       â””â”€â”€> Timestamped snapshot
    â”‚
    â””â”€â”€> Save to outputs/scoring_results_flat.csv
            â””â”€â”€> Flattened for Excel/analysis
                - resume_path
                - job_description_path
                - total_score
                - {criterion}__raw_score
                - {criterion}__weight_given
                - {criterion}__normalized_percentage
                - {criterion}__weighted_contribution
                - missing_skills
                - experience_gap_years
                - education_gap
```

## ðŸ§© Component Details

### Services Layer

#### `services/llm.py` - LLMService
- **Purpose**: Wrapper around LangChain + Google Gemini
- **Key Methods**:
  - `__init__(model, temperature)`: Initialize with API key from environment
  - `generate(system_prompt, human_prompt)`: Generate LLM response
- **Dependencies**: `langchain`, `langchain-google-genai`, `python-dotenv`

#### `services/pdf_utils.py` - PDF Text Extraction
- **Purpose**: Extract text from PDF files
- **Key Functions**:
  - `extract_text_from_pdf(pdf_path)`: Extracts all text from PDF pages
- **Dependencies**: `PyPDF2`
- **Error Handling**: Raises `FileNotFoundError` if PDF missing, `ValueError` if no text found

### Agents Layer

#### `agents/job_description_parser_agent.py` - JobDescriptionParserAgent
- **Input**: PDF path
- **Output**: `JobCriteria` (Pydantic model)
- **Process**:
  1. Extract PDF text
  2. Send to LLM with structured prompt
  3. Parse JSON response
  4. Validate and return `JobCriteria`

#### `agents/resume_screener_agent.py` - ResumeScreenerAgent
- **Input**: PDF path
- **Output**: `ResumeData` (Pydantic model)
- **Process**:
  1. Extract PDF text
  2. Send to LLM with extraction prompt
  3. Parse JSON response (skills, education, experience)
  4. **Experience Calculation**:
     - LLM calculates from employment dates
     - Full-time work = 1x duration
     - Internships = 0.5x duration
     - Returns as float (e.g., 2.5 years)
  5. Validate and return `ResumeData`

#### `agents/matchmaker_agent.py` - MatchmakerAgent
- **Input**: `ResumeData`, `JobCriteria`
- **Output**: `MatchResult` with `MatchScores`
- **Scoring Logic**:
  - **Skills Match**: `(job_skills âˆ© resume_skills) / job_skills * 100`
  - **Experience Match**: 
    - 100 if resume_exp >= job_required OR job doesn't require experience
    - 40-90 (proportional) if resume_exp > 0 but < required
    - 30 if resume_exp = 0 but job requires
  - **Education Match**: 
    - 100 if resume_edu >= job_edu
    - 70 if resume_edu < job_edu
  - **Overall**: `0.6 * skills + 0.25 * experience + 0.15 * education`

### Scoring Layer

#### `structured_scoring_agent.py` - StructuredScoringAgent
- **Purpose**: LLM-based comprehensive scoring with weighted criteria
- **Key Methods**:
  - `normalize_weights(criteria_requirements)`: Normalizes weights to percentages
  - `create_scoring_prompt()`: Single resume scoring prompt
  - `create_batch_scoring_prompt()`: Batch comparative scoring prompt
  - `score_resume()`: Score single resume (backward compatibility)
  - `score_resumes_batch()`: **Primary method** - scores all resumes together
  - `parse_llm_response()`: Extract JSON from LLM response
  - `format_results()`: Pretty-print results

**Batch Scoring Benefits**:
- Reduces hallucination through relative comparison
- More consistent scoring across candidates
- Single LLM call (faster, cheaper)

**Criteria Evaluated**:
- Technical Skills
- Experience
- Education
- Presentation
- Certifications (optional)
- Projects (optional)
- Soft Skills (optional)
- Industry Knowledge (optional)

### Models Layer

#### `common/models.py` - Pydantic Models

**JobCriteria**:
```python
- position: str
- required_skills: List[str]
- preferred_skills: List[str]
- min_experience_years: int
- education_level: str
- industry: str
- company_size: str
- remote_work: bool
```

**ResumeData**:
```python
- file_path: str
- raw_text: str
- extracted_skills: List[str]
- education_level: str
- total_experience_years: float  # Parsed from resume dates
```

**MatchScores**:
```python
- skills_match: int (0-100)
- experience_match: int (0-100)
- education_match: int (0-100)
- overall_score: int (0-100)
```

**MatchResult**:
```python
- job: JobCriteria
- resume: ResumeData
- scores: MatchScores
- analysis_notes: Dict[str, Any]
```

## ðŸ”€ Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job PDF     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JobDescriptionParser    â”‚â”€â”€â”€â”€â”€â–¶â”‚  JobCriteria â”‚
â”‚   Agent                 â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚ Resume PDFs  â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
       â”‚                                â”‚
       â–¼                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ResumeScreenerAgent     â”‚      â”‚             â”‚
â”‚   (per resume)          â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚             â”‚
       â”‚                          â”‚             â”‚
       â–¼                          â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ResumeData   â”‚â”€â”€â”€â”€â”€â–¶â”‚ MatchmakerAgent              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   (rule-based matching)      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   MatchResult[]      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ (for gap analysis)
                                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                          â”‚                          â”‚
       â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StructuredScoring   â”‚  â”‚  Gap Analysis        â”‚  â”‚   Output Files   â”‚
â”‚ Agent (Batch)       â”‚  â”‚  Calculation         â”‚  â”‚   JSON & CSV      â”‚
â”‚  (LLM evaluation)   â”‚  â”‚                      â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Final Results â”‚
                            â”‚   (Ranked)    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸŽ›ï¸ Configuration Flow

```
criteria_requirements.json
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ load_criteria_from_   â”‚
â”‚   file()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Merge scoring_ â”‚
    â”‚ criteria +      â”‚
    â”‚ additional_     â”‚
    â”‚ criteria        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ normalize_weights()     â”‚
â”‚   (auto-normalize)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Pass to LLM   â”‚
    â”‚   in prompt    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš¨ Error Handling

### Resume Parsing Errors
- **PDF extraction fails**: Skips resume, logs error
- **LLM parsing fails**: Uses default values (empty skills, "bachelors", 0.0 experience)
- **JSON parse error**: Falls back to defaults

### Job Parsing Errors
- **PDF extraction fails**: Raises exception (pipeline stops)
- **LLM parsing fails**: Raises exception (pipeline stops)
- **JSON parse error**: Raises exception (pipeline stops)

### Batch Scoring Errors
- **Batch LLM call fails**: Falls back to individual scoring (`_fallback_individual_scoring`)
- **Response parsing fails**: Falls back to individual scoring
- **Individual scoring fails**: Marks result as `{"success": False, "error": "..."}`

### Output Errors
- **JSON save fails**: Logs warning, continues
- **CSV save fails**: Logs warning, continues
- Results are still displayed in console

## ðŸ”§ Extensibility Points

1. **Add New Criteria**: Edit `criteria_requirements.json` and update prompts
2. **Custom Scoring Logic**: Modify `MatchmakerAgent.score()`
3. **Different LLM**: Change model in `services/llm.py` or pass to agents
4. **Additional Agents**: Create new agent classes following existing patterns
5. **Custom Output Format**: Modify `_flatten_scoring_result()` in `pipeline.py`

## ðŸ“Š Performance Considerations

- **Batch Scoring**: Reduces LLM calls from N (one per resume) to 1
- **Parallel PDF Extraction**: Could be parallelized (currently sequential)
- **Caching**: No caching implemented (each run calls LLM)
- **Token Usage**: Batch scoring uses more tokens per call but fewer total calls

## ðŸ” Security Notes

- API keys stored in `.env` (not committed)
- PDF paths are user-provided (validate in production)
- LLM responses are parsed and validated (Pydantic models)
- No input sanitization for file paths (assumes trusted environment)

